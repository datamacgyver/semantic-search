{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Semantic search\n",
    "The below notebook is a really quick and dirty example for how one could implement a semantic search engine using `sentence_transformers` (part of [sbert](https://www.sbert.net/index.html)) and Opensearch (AWS' Elasticsearch implementation).\n",
    "\n",
    "## Glossary\n",
    "* Model - Basically a collection of mathematical functions that can predict or transform data. These usually require training in some way so they can understand the world\n",
    "* Embedding - A word or sentence converted to numbers, usually stored as a list or vector\n",
    "* Distance - In this context, this is how far two sets of numbers are from one another. Related to:\n",
    "    * K Nearest Neighbours - K being the amount of nearby values to return\n",
    "    * Dot product and cosine similarity - ways of evaluating distance\n",
    "\n",
    "## Semantic what?\n",
    "Okay, so what I mean is \"searching based on meaning, not synonyms\". The models used below try to extract the actual meaning in a sentence and represent it mathematically as a list of numbers. Two lists that are similar to each other will have similar numbers (which we often refer to as distance). Bigger models will use longer lists of numbers for this but will also take longer to encode and match, it's a trade off that I've not really explored here, preferring to just see how well different models work.\n",
    "\n",
    "In the olden days, this kind of thing just used single words but that often failed to catch the meaning behind things, glasses for example could be for your face or for your wine, it's all about context. Later attempts combined the scores for all the words in a sentence to get the aggregate meaning, but this often led to an over-representation of junk words such as `the` or `and` (or `or` for that matter!) so more recently models have been built that will actually take the whole sentence in and try to get an aggregate meaning. I've not done the reading to decide which model does what in the below but I have focused on sentence based approaches. This is a double edged sword in that we will probably understand the meaning of the sentence better but our vectors will be bigger (and thus more expensive to create and search), and also it may mean that match scores can be low (as there's more meaning in the weight bench description than just `gym`). The latter of these should be fine as `gym` will still score more highly than something nonesensical, such as `cat`.\n",
    "\n",
    "## How could this look on AWS?\n",
    "Read [this AWS post](https://aws.amazon.com/blogs/machine-learning/building-an-nlu-powered-search-application-with-amazon-sagemaker-and-the-amazon-es-knn-feature/) for an example. This is basically what I've done here but hosts the encoding of the sentences to numbers on Sagemaker. The advantage of this is SM has endpoints that are specifically adapted to run rapid predictions from complex models (called Elastic Endpoints), meaning you will likely get results faster than using a lambda/ECS container of similar size.\n",
    "\n",
    "## Methodology\n",
    "I've loaded up two product descriptions from Amazon.co.uk, one for a tent and one for a weights bench. I've then tried a few different models (see below sections for details) and also tried them using the whole description and also just the nouns from the description. The advantage of stripping nouns is that the things we are looking for are likely nouns so you are less likely to get non-target matches. The disadvantage is that there is less context to learn.\n",
    "\n",
    "My queries are just randomly chosen words. Some are in the descriptions, some are not but are related to them, and others are just randomly chosen words from deep in my subconscious, you can make what you will of the fact `bra` is in there. Both the descriptions and the queries are encoded to list of numbers (often called vectors) and then their distance is compared.\n",
    "\n",
    "\n",
    "## Warnings\n",
    "* I've not done this on lots of data, just the two examples below, if you've got a load of descriptions then hit me up and we can play!\n",
    "* I have a GPU so this runs really fast. You mileage in production may vary, if you need it faster then use a smaller model (but you may lose accuracy in doing so, it's a trade off)\n",
    "* A simple word embedding using extracted nouns may be the fastest option and I've not done that here, preferring sentence embeddings. As with the above point, by losing the sentences you lose some meaning. The resultant search would struggle to differentiate glasses on your face and glasses for your wine!\n",
    "* You may need to play with Opensearch's settings to get the right distance metrics for your model, also not done here.\n",
    "\n",
    "## Right tools, right job\n",
    "AWS Kendra will also do this for you but it's not cheap (£1,000> a month), reading the brief it also seems optimised for internal searches over products but I've not tested this.\n",
    "\n",
    "It's important to state that this is for search, if your problem is classifying products into categories, you could use searches on those names and deccide on a confidence cutoff but that may get clunky, a better option may be AWS Comprehend if you have time and money. Similarly, if your problem is one of \"what does this person want to buy\" you may get more luck from AWS Personalize [sic] which will do variations on product recommendation systems for you. This can group similar people and similar products together.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from textblob import TextBlob\n",
    "import json\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "## On first run you may have to uncomment these three and run them to get everything installed. I think this is just needed for the noun stripping so if you don't need that you don't need this.\n",
    "# import nltk\n",
    "# nltk.download('brown')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [],
   "source": [
    "psg_gym_bench = 'The BodyMax CF302 Flat Bench with Dumbbell Rack allows you to create an exciting and varied workout to help strengthen, tone and promote weight loss, whilst also keeping your gym floor clutter free! Stylish and classic, this flat bench has been thoughtfully constructed with durable upholstery and high-density padding to ensure maximum comfort while you train.'\n",
    "\n",
    "psg_tent = \"The Theta 4 Tent is a four-person tunnel tent, uniquely featuring two sleeping cabins and a moveable front wall, allowing you to choose whether you have a covered porch or a larger living area inside your tent. With such an incredible size of 340(W) x 480(D) x 190cm(H), the Theta 4 has more than plenty of space inside – perfect for a family getaway, a couples’ retreat, or a trip shared among friends.\"\n",
    "\n",
    "queries = [\"gym\", \"holiday\", \"bench\", \"bra\", \"television\", \"exercise\", \"exercise equipment\",\n",
    "           \"weights bench\" , \"rower\", \"weights\", \"tent\", \"outdoors\", \"camping\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# What do the vectors look like?\n",
    "We get a vector of encoded values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-2.98966765e-01, -9.48199153e-01,  7.11256266e-01,\n         2.05562741e-01, -1.15234107e-01, -6.24987409e-02,\n         4.88208443e-01, -6.75297797e-01,  4.36219543e-01,\n         6.65203631e-01, -4.30135369e-01, -3.42238784e-01,\n         1.77010521e-01,  4.70797688e-01,  4.98695493e-01,\n        -1.69574451e+00,  6.59173250e-01,  5.87425470e-01,\n         2.10393772e-01, -1.13293862e+00, -4.81042340e-02,\n        -2.24366769e-01, -8.57433259e-01, -1.25133109e+00,\n        -3.93987559e-02,  2.50246018e-01,  1.27295303e+00,\n        -1.40825853e-01, -3.17997187e-02,  7.64016688e-01,\n        -8.09232295e-01, -1.09921552e-01, -8.69391412e-02,\n        -5.66437125e-01,  3.18861723e-01,  5.16373754e-01,\n        -2.12678656e-01, -3.96210134e-01, -9.86271858e-01,\n        -1.53267562e-01, -3.01156163e-01, -3.55849564e-01,\n         2.60932185e-02,  8.12458932e-01,  3.03995848e-01,\n         6.91898763e-02,  4.52562124e-02, -2.54531503e-01,\n         4.96325552e-01,  8.66791829e-02,  7.63809383e-01,\n         4.65751290e-02, -8.10524702e-01, -2.88107276e-01,\n        -7.72791505e-02, -7.54941225e-01, -8.66971016e-01,\n        -8.55843186e-01, -6.49880543e-02,  4.07951504e-01,\n        -4.90929335e-01,  6.29211187e-01, -1.52346641e-01,\n         4.20872539e-01,  7.67164767e-01,  6.74610615e-01,\n         1.62617609e-01,  5.84825814e-01, -2.24342033e-01,\n        -6.11955106e-01,  2.63422340e-01,  3.37756217e-01,\n        -2.15804875e-01, -2.29039383e+00, -6.92054536e-03,\n         5.48039190e-03, -1.39050436e+00,  4.26071137e-01,\n        -1.11237898e-01,  3.26982081e-01,  7.59695917e-02,\n         1.68215379e-01, -3.47396225e-01,  1.31097734e-01,\n        -1.09673715e+00, -3.57107610e-01, -3.17096114e-01,\n         3.61296609e-02, -8.35079178e-02, -5.58944106e-01,\n        -7.34167933e-01,  7.57266581e-02, -4.48444963e-01,\n         2.34322339e-01,  5.85081100e-01, -1.77851349e-01,\n        -7.70466089e-01,  5.63576281e-01,  4.77914661e-01,\n        -6.30446672e-01,  4.32948381e-01,  5.17420378e-03,\n         1.98481053e-01,  1.84059590e-01,  7.00693846e-01,\n         1.66742945e+00, -4.20512795e-01, -6.00669645e-02,\n         1.24566503e-01,  3.05840760e-01, -4.54176158e-01,\n        -9.77955535e-02,  4.39318985e-01,  1.28468081e-01,\n        -1.90611184e-01, -1.36526614e-01, -4.92777824e-01,\n         3.06039117e-02,  7.40433931e-02, -2.74854928e-01,\n         3.30812961e-01,  2.92679109e-02, -8.47029388e-01,\n         5.12503445e-01,  6.01997852e-01, -2.93863583e-02,\n         5.34957051e-01, -1.35615319e-01,  3.71067435e-01,\n        -3.36866360e-03, -5.44027686e-02, -6.93816245e-01,\n        -1.95903823e-01,  4.04547155e-01,  8.12786281e-01,\n        -5.00887558e-02, -1.43531501e-01, -3.56233865e-01,\n         4.32200640e-01,  2.24210739e-01, -8.33470881e-01,\n        -2.84539789e-01, -4.11366940e-01,  4.05828685e-01,\n        -9.40289199e-01,  6.56428933e-01, -2.27658927e-01,\n        -3.89662445e-01, -1.42685007e-02,  4.76468295e-01,\n        -6.11394584e-01,  8.22011948e-01,  4.55692977e-01,\n         2.10355282e-01,  5.02608359e-01, -1.36675119e+00,\n        -8.17831814e-01,  2.20150277e-01,  2.98232734e-01,\n         8.42396617e-02, -2.15209693e-01,  5.67801714e-01,\n         6.04178071e-01, -1.69007868e-01, -1.63276210e-01,\n         4.37724799e-01,  6.82091534e-01,  7.19442889e-02,\n        -3.20050180e-01, -2.04310730e-01, -4.24257755e-01,\n         5.76772928e-01, -6.28956705e-02, -5.77972174e-01,\n         3.45820338e-01, -5.50001502e-01, -2.26504222e-01,\n         1.30334091e+00, -2.99606442e-01, -9.70284343e-02,\n         2.55076110e-01,  1.01285487e-01,  2.04245541e-02,\n         5.77693284e-01,  6.49630785e-01, -5.01820982e-01,\n         6.79022312e-01,  2.90781945e-01,  3.20086718e-01,\n         1.74019814e-01,  2.13762134e-01,  2.55410612e-01,\n         5.72155058e-01, -1.00783741e+00, -6.04947209e-01,\n         5.64270437e-01, -3.05247456e-01, -1.81416512e-01,\n         1.57396361e-01, -5.47430694e-01, -7.45918572e-01,\n        -9.30260777e-01,  8.63944590e-02, -2.11060911e-01,\n        -6.13194466e-01, -3.86932731e-01,  1.88036764e-03,\n         4.92078304e-01,  2.18317464e-01,  8.68015811e-02,\n        -3.30321699e-01,  5.05653739e-01, -1.13645196e-02,\n        -7.00638235e-01, -9.96197462e-01,  8.76542807e-01,\n        -2.31181011e-01,  1.59474090e-01,  1.03428173e+00,\n        -3.47127557e-01,  3.68781298e-01, -4.76262718e-01,\n        -3.31473529e-01, -3.36772978e-01,  5.55791438e-01,\n        -1.07343286e-01, -2.82645285e-01,  2.71766186e-01,\n         7.09159017e-01, -1.94128573e-01,  1.92526281e-01,\n        -3.71296763e-01, -6.17603540e-01, -8.97305533e-02,\n        -3.63549352e-01,  7.80122308e-03,  3.44877362e-01,\n         2.54942179e-01, -6.89289123e-02, -4.66854930e-01,\n        -5.34190416e-01,  3.12698223e-02, -2.64914423e-01,\n         1.25103980e-01,  7.57310688e-02,  4.18471277e-01,\n         4.07923341e-01, -5.27080521e-02, -3.57834965e-01,\n        -3.18028867e-01, -2.72835732e-01, -3.24783236e-01,\n         1.54794142e-01, -9.70701501e-02,  3.76191884e-01,\n        -6.26048148e-02, -7.14929253e-02, -3.56673539e-01,\n         3.94770324e-01, -1.08996820e+00, -5.17033398e-01,\n         1.72485515e-01, -2.83423364e-01,  2.22089037e-01,\n        -5.99023521e-01,  5.73519945e-01, -2.58157939e-01,\n        -5.31949461e-01, -6.33739769e-01, -2.39584684e-01,\n         5.41827738e-01, -8.38904083e-01, -2.85338044e-01,\n         1.29306364e+00,  9.44064558e-02,  3.99985164e-02,\n         9.60590124e-01, -1.25558794e-01,  9.94948000e-02,\n        -5.39143622e-01,  6.09676600e-01, -6.15806341e-01,\n         9.21351194e-01,  1.56757891e-01,  3.04808825e-01,\n         6.73799813e-01,  4.89437521e-01, -2.29503095e-01,\n         2.07037628e-01, -2.72904545e-01, -2.32196227e-01,\n        -7.14712024e-01, -4.77929592e-01, -4.22580838e-01,\n         5.97423673e-01, -8.75660360e-01,  2.34770134e-01,\n         1.03371823e+00,  7.53881363e-03, -6.07997060e-01,\n         4.57927078e-01,  4.60807532e-01, -2.42002800e-01,\n        -5.86032331e-01, -1.42191481e-02,  1.96498454e-01,\n        -6.65014029e-01, -3.10443819e-01, -5.26761591e-01,\n        -7.48785019e-01, -7.72591829e-02, -6.04656599e-02,\n         5.90864778e-01, -8.13107938e-03,  1.03125922e-01,\n         1.83826208e-01, -9.87532064e-02,  6.62034273e-01,\n         1.65728822e-01, -7.15761662e-01, -1.33722320e-01,\n         1.82435274e-01,  6.09345675e-01, -4.88392472e-01,\n        -5.39260097e-02, -3.90833020e-01, -2.30242908e-01,\n        -5.01553357e-01,  1.52699098e-01, -7.56673887e-02,\n        -1.06388234e-01, -6.39427975e-02,  4.60309386e-01,\n         4.18986892e-03,  2.96807438e-01,  8.20181370e-02,\n        -1.18228006e+00,  4.52749729e-01, -2.78171390e-01,\n        -1.08771855e-02, -4.76068556e-01, -8.32086980e-01,\n         1.84966847e-01,  6.34509742e-01,  5.41747510e-01,\n         4.61110651e-01,  1.07976997e+00, -1.07953034e-01,\n        -6.44563615e-01,  2.45918989e-01,  3.58556062e-01,\n        -1.20457420e-02,  6.76499903e-02,  6.32593989e-01,\n         6.02958143e-01,  2.98284125e-02, -2.52817333e-01,\n         1.12454019e-01,  1.38546005e-01,  6.20147645e-01,\n         3.60268429e-02, -4.19557951e-02,  2.72080511e-01,\n         3.76850903e-01, -1.22846532e+00,  2.26079673e-01,\n        -7.41459787e-01, -2.13322431e-01, -8.62206161e-01,\n         1.85271710e-01, -3.93713683e-01, -1.97454482e-01,\n        -1.76849335e-01,  1.66365576e+00,  1.57274410e-01,\n        -4.84792292e-01, -5.99760234e-01, -4.31397110e-01,\n         3.53139251e-01, -3.66064813e-03, -4.78552669e-01,\n         6.10117793e-01, -5.01228310e-02, -1.26583531e-01,\n        -3.69484842e-01, -8.37256968e-01, -5.02772391e-01,\n         8.56572032e-01,  9.35254842e-02, -4.69348848e-01,\n        -5.85123487e-02,  8.22686851e-01,  5.65976679e-01,\n         3.70621443e-01,  5.40757775e-02, -3.51829171e-01,\n        -6.02071941e-01,  8.46861824e-02,  9.57659408e-02,\n        -3.00890595e-01, -4.96373594e-01,  4.60656315e-01,\n         6.82529211e-01,  9.65187907e-01,  5.22682488e-01,\n        -3.07553530e-01,  1.07262754e+00,  8.41229707e-02,\n         2.77533382e-01, -1.74135923e-01, -6.03791952e-01,\n         3.77557635e-01, -3.15526754e-01,  5.19647777e-01,\n        -3.83989990e-01, -3.20917547e-01, -7.91077241e-02,\n         1.20887506e+00, -7.53030181e-01, -1.30770683e-01,\n        -3.88308316e-02, -4.87910420e-01,  2.18933314e-01,\n         3.84259343e-01, -3.64536434e-01,  3.64857078e-01,\n         9.41901207e-02, -2.32924566e-01,  5.27196407e-01,\n         1.44787061e+00,  6.89302012e-02, -3.57737750e-01,\n         3.35090309e-01, -3.35236132e-01,  6.07039928e-01,\n        -7.76787922e-02,  7.08124638e-01,  1.01502858e-01,\n        -4.71942782e-01,  8.04296255e-01, -4.80485223e-02,\n        -3.42179358e-01,  5.63605011e-01,  5.90284169e-01,\n         2.01382600e-02,  8.18636417e-02,  7.35132396e-02,\n         1.31254613e-01, -8.76162291e-01,  3.79321635e-01,\n        -5.32714725e-01,  3.86890918e-01,  6.17381990e-01,\n        -1.03247888e-01,  2.94146657e-01,  3.92069258e-02,\n        -3.48558336e-01,  5.11382878e-01, -9.09880638e-01,\n         1.00066915e-01,  6.90410197e-01,  5.71793139e-01,\n        -2.93520153e-01,  2.00600550e-01, -2.03811184e-01,\n         4.77671772e-01, -2.91177124e-01, -5.18668033e-02,\n        -1.14553750e+00,  2.77134590e-02, -1.69033065e-01,\n        -3.08018059e-01,  7.71354973e-01, -2.09570732e-02,\n        -3.95061255e-01,  5.51071405e-01, -2.90036470e-01,\n        -9.23779249e-01,  6.77693427e-01,  3.89772415e-01,\n         2.82677382e-01, -3.46970499e-01,  4.33071047e-01,\n         6.88861310e-01,  3.17763507e-01, -3.78355324e-01,\n         5.26012301e-01, -1.60154745e-01,  2.62123823e-01,\n        -2.42468655e-01, -2.26916522e-01, -3.44121158e-01,\n        -2.16338873e-01,  3.76236707e-01,  8.60313103e-02,\n        -5.75243056e-01,  7.23212302e-01, -1.32583469e-01,\n        -6.97136045e-01, -4.01407063e-01,  3.63435410e-02,\n        -1.08505332e+00, -1.14500129e+00,  1.52966225e+00,\n        -5.67560196e-01,  9.36252177e-01,  9.92966652e-01,\n         2.91752338e-01, -5.28681517e-01, -2.56460994e-01,\n        -2.05777407e-01,  1.97019875e-01,  5.01932859e-01,\n        -2.04910934e-01,  8.56197700e-02,  1.16034970e-01,\n        -3.55496407e-02,  4.47067320e-02,  1.49895620e+00,\n         7.92098045e-01,  4.43193316e-01,  5.72762787e-01,\n        -2.34563217e-01, -1.09160490e-01,  4.60561007e-01,\n         3.74386489e-01,  6.56857848e-01, -3.09249997e-01,\n         4.96136621e-02, -7.47024953e-01,  6.76027954e-01,\n         7.25089908e-01,  3.61596584e-01,  7.54295588e-01,\n         3.17248225e-01,  8.49763602e-02,  4.23864365e-01,\n        -7.42202640e-01,  3.15624028e-01,  8.52774736e-03,\n        -5.06381571e-01, -3.11958402e-01, -2.27327958e-01,\n        -7.32553422e-01,  2.45490685e-01,  4.09349203e-02,\n        -1.02683663e+00, -3.90442371e-01,  1.09396666e-01,\n         6.26726389e-01, -4.38298315e-01,  2.20430031e-01,\n        -5.40698528e-01,  3.22327107e-01, -4.33519244e-01,\n         2.65201628e-01,  6.44301295e-01,  4.53428328e-01,\n        -1.62888095e-01,  1.19828619e-01, -6.92126572e-01,\n         5.56965351e-01, -1.73689172e-01,  3.00633460e-01,\n         5.87650180e-01, -5.15523314e-01, -1.39156923e-01,\n         6.28287792e-02,  4.71783102e-01, -4.67844337e-01,\n        -2.40231216e-01,  2.80370623e-01,  3.03180039e-01,\n        -1.05829366e-01,  5.14186084e-01,  5.40956438e-01,\n         5.53629637e-01,  4.21803355e-01,  6.41882002e-01,\n        -2.60502547e-01,  3.04644629e-02,  5.97697735e-01,\n         1.81684703e-01,  9.60985184e-01,  6.46459311e-02,\n         7.14008451e-01,  1.02590120e+00,  8.66224989e-02,\n         5.24458230e-01,  1.10592842e-01,  5.96338749e-01,\n        -5.04707932e-01, -3.58685199e-03,  1.83721960e-01,\n         1.80699006e-01,  3.33261967e-01,  3.78754437e-01,\n         6.45861551e-02, -1.25125781e-01, -2.90078998e-01,\n        -1.07650137e+00, -1.15515128e-01,  2.17016354e-01,\n        -2.24235952e-01, -2.74511814e-01,  1.50259942e-01,\n         1.96281955e-01, -5.62952161e-01, -5.73232770e-01,\n        -6.94686696e-02, -5.75320244e-01,  3.68326247e-01,\n        -1.57436088e-01, -8.33346128e-01, -4.55587059e-01,\n         2.46412545e-01,  4.50780332e-01,  4.26474512e-01,\n        -5.47792427e-02, -9.75690782e-02, -2.34350577e-01,\n        -9.51063693e-01,  1.52622294e-02, -1.21595979e+00,\n        -9.38608825e-01,  4.44113374e-01, -4.14659202e-01,\n        -3.01696181e-01,  2.42947400e-01,  6.97198629e-01,\n        -2.43700579e-01, -1.47774415e-02, -6.50118828e-01,\n        -7.79634863e-02, -5.80815613e-01,  3.65348011e-02,\n        -5.75665891e-01, -8.68213713e-01, -5.91942549e-01,\n         4.90461349e-01, -1.32381558e-01, -5.78820288e-01,\n         7.69175813e-02,  7.07867503e-01, -1.95712298e-01,\n        -6.28755093e-02,  1.04442489e+00, -1.32225406e+00,\n         1.35429591e-01,  4.21354651e-01,  7.60378420e-01,\n        -6.66557968e-01, -6.80253565e-01,  1.96498796e-01,\n         5.73846281e-01, -2.65533552e-02, -4.22805309e-01,\n         2.28491411e-01,  1.05283871e-01, -2.16480777e-01,\n        -5.17228484e-01,  7.15380430e-01,  3.64044249e-01,\n         3.52086484e-01,  7.20541850e-02,  6.54098213e-01,\n        -7.16502905e-01,  1.45125955e-01, -3.03337455e-01,\n        -1.96567215e-02,  3.47428352e-01, -4.17943805e-01,\n        -5.29380083e-01,  2.12363914e-01,  1.21086287e+00,\n         2.25867331e-01, -6.89658582e-01,  3.49600822e-01,\n        -7.90180266e-01, -2.28311032e-01, -4.19046462e-01,\n         2.69205458e-02, -3.06604475e-01,  1.36555135e-01,\n        -1.23009510e-01, -1.40792227e+00, -1.34006545e-01,\n        -4.19219226e-01, -5.70766270e-01, -9.05092895e-01,\n        -4.02284950e-01,  8.66058946e-01,  1.13401264e-01,\n         1.27876967e-01,  4.30461466e-01,  4.89741325e-01,\n         2.51454562e-01, -1.96178526e-01, -1.00486290e+00,\n         3.11369300e-01,  1.79095656e-01, -1.95664153e-01,\n         4.77455199e-01,  1.18355501e+00,  5.23144960e-01,\n         5.97805977e-02, -6.02901578e-01,  1.06154412e-01,\n         8.56896877e-01, -2.47455567e-01, -2.85090804e-01,\n        -1.94710139e-02, -1.48343220e-01, -1.06914878e-01,\n        -3.60397220e-01,  2.68359870e-01, -9.15988088e-01,\n        -4.16319013e-01,  3.77588630e-01,  7.61917353e-01,\n         6.75169602e-02,  7.42643952e-01,  2.95633197e-01,\n        -2.97826439e-01, -1.36908382e-01,  1.53252304e-01,\n        -6.58567905e-01, -5.60148954e-01, -4.65829223e-01,\n        -5.07094145e-01, -8.51645917e-02, -8.37306306e-03,\n        -5.52384019e-01,  1.02785444e+00, -7.80555308e-01,\n        -7.07557082e-01, -2.47041762e-01,  4.57026333e-01,\n        -4.29094195e-01,  3.34361643e-01, -3.55931580e-01,\n        -4.28814953e-03,  5.95206976e-01,  2.25356072e-02,\n         2.37608016e-01, -2.82839805e-01, -6.12238884e-01,\n         2.65974939e-01,  2.65773743e-01, -6.99485466e-02,\n         2.70871073e-01, -9.83117223e-01, -1.76988512e-01,\n        -9.93287712e-02, -9.25175771e-02,  1.11426383e-01,\n         4.68886733e-01,  3.75508845e-01,  9.31974113e-01,\n         3.21620584e-01,  6.16804481e-01, -3.41671586e-01,\n        -8.15456435e-02, -3.41461715e-03,  4.94028568e-01,\n        -7.39629090e-01,  2.13486120e-01,  3.89442146e-01]], dtype=float32)"
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = SentenceTransformer('msmarco-distilroberta-base-v2')\n",
    "m.encode([psg_gym_bench])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-9.26285833e-02,  1.23928711e-01, -8.58291239e-03,\n         4.32949454e-01,  9.17690754e-01, -2.59607404e-01,\n         3.43610346e-01, -6.77743852e-02, -2.22118720e-01,\n        -6.31535172e-01,  8.11305642e-01,  2.74949968e-01,\n        -3.35835099e-01, -6.51537597e-01,  1.66350052e-01,\n         1.74393320e+00, -5.59948850e-03,  6.49918497e-01,\n         4.67403710e-01, -2.20262975e-01,  1.85428739e-01,\n         2.50346541e-01, -2.95338959e-01, -4.50864553e-01,\n         2.35834390e-01,  4.25461173e-01, -1.42581612e-01,\n         6.00255966e-01,  1.74672127e-01,  3.39640938e-02,\n        -3.40345502e-01,  3.11843097e-01, -9.92195308e-02,\n         5.08702934e-01,  6.32673860e-01, -4.04084884e-02,\n        -2.41434857e-01,  1.60248131e-01,  1.14822209e+00,\n        -1.24567570e-02,  3.94627154e-01,  2.04300225e-01,\n         6.77203089e-02,  2.79878020e-01,  1.86520949e-01,\n        -1.79881111e-01,  1.92308962e-01,  1.12073135e+00,\n        -6.96272328e-02,  1.54915273e-01, -4.40835834e-01,\n        -3.77066612e-01,  1.26982480e-01,  5.91526031e-01,\n        -1.78282320e-01, -4.56254780e-01, -2.01287940e-01,\n         3.68831486e-01,  2.81289034e-02, -3.58875692e-01,\n        -6.37270749e-01,  1.11190736e+00,  5.33815026e-01,\n        -8.92706335e-01, -5.22755384e-01,  5.23293316e-01,\n        -4.77525979e-01, -3.06229353e-01,  4.13062423e-01,\n        -2.23520488e-01, -7.68955052e-01,  4.74078320e-02,\n        -2.86203563e-01, -4.11095858e-01, -3.11158627e-01,\n         2.44803682e-01, -6.13719583e-01,  2.04260015e+00,\n        -1.54368424e+00, -2.03468710e-01,  3.05309415e-01,\n         6.74756408e-01,  6.69468284e-01,  2.27307379e-01,\n        -7.62926936e-01,  2.37825476e-02, -3.15222703e-03,\n         1.97631761e-01,  2.98176378e-01,  3.64239991e-01,\n        -1.75442412e-01, -6.13465160e-02, -8.75345051e-01,\n         2.96619564e-01, -5.42362571e-01, -9.44671273e-01,\n        -6.80010915e-01,  1.18573949e-01,  3.40368241e-01,\n         2.72404194e-01, -2.96586417e-02,  4.85258102e-01,\n        -5.33509493e-01, -1.81859612e-01,  6.98193908e-02,\n         2.25388959e-01,  2.09294915e-01, -6.87620193e-02,\n         5.43192104e-02,  2.75438912e-02,  1.23388730e-02,\n        -1.09115851e+00, -5.66819429e-01, -8.45741406e-02,\n        -5.33034861e-01,  4.63252664e-01, -8.61500263e-01,\n        -1.04014866e-01, -5.64235091e-01, -2.38909036e-01,\n         2.97366172e-01,  7.19223678e-01, -1.49249601e+00,\n        -6.67067841e-02, -5.59235513e-01, -3.74995857e-01,\n        -4.88664985e-01,  3.33310485e-01,  1.12433873e-01,\n        -3.09647202e-01,  1.38825759e-01, -1.16520119e+00,\n         5.50467014e-01,  1.11019298e-01,  1.68774277e-01,\n        -1.91415220e-01, -1.17668018e-01, -3.60286869e-02,\n         1.36106431e-01, -2.89157778e-01,  9.28065181e-02,\n         1.77448481e-01,  3.45978111e-01,  3.48607630e-01,\n        -9.47789311e-01, -3.47695708e-01, -1.21907793e-01,\n         1.46934867e-01,  1.77754521e-01,  1.84284836e-01,\n         3.95468742e-01, -3.07001650e-01,  5.17536104e-02,\n         1.11789042e-02,  1.03545880e+00,  3.91105235e-01,\n        -3.32465261e-01, -5.73320389e-01, -1.38028219e-01,\n        -2.42121935e-01,  5.37012145e-03, -5.28642014e-02,\n        -1.38088301e-01,  9.47150290e-02,  1.26356184e-01,\n        -1.26021598e-02,  2.95446932e-01, -1.79372311e-01,\n        -6.57659650e-01,  6.69937789e-01, -2.95847654e-01,\n        -5.15565813e-01,  1.90780371e-01, -5.75159609e-01,\n         7.11693019e-02, -1.42392099e-01, -2.89194286e-01,\n         9.12037566e-02,  6.06665239e-02, -1.15679339e-01,\n        -6.17907643e-02,  1.25741065e-01,  2.22612649e-01,\n         9.05965790e-02, -5.34780383e-01, -2.16021597e-01,\n        -2.09942371e-01,  2.74016321e-01,  7.55591273e-01,\n         6.24367237e-01,  2.39706337e-01,  8.61968622e-02,\n         2.38885731e-02, -5.48009574e-01,  1.70398839e-02,\n         1.22302711e-01,  4.21653986e-01,  2.36750945e-01,\n         6.11354947e-01,  1.89300999e-01,  1.82868719e-01,\n        -5.82350492e-01,  1.09131432e+00,  8.18825439e-02,\n         5.38861394e-01, -3.95424873e-01,  3.09420794e-01,\n        -2.75034070e-01, -2.66398132e-01, -7.44557798e-01,\n         3.82032067e-01, -1.34458113e+00, -6.11383319e-01,\n        -3.75150651e-01,  1.08712971e+00, -5.35294533e-01,\n        -4.59722914e-02,  9.99556065e-01, -1.12611973e+00,\n         4.54344273e-01,  3.78859565e-02,  1.61739975e-01,\n         4.01265085e-01, -1.64863527e-01,  7.63503388e-02,\n        -7.21858621e-01, -2.52907097e-01, -1.88496727e-02,\n         2.71386325e-01, -1.05348432e+00,  3.70967537e-02,\n         4.93057191e-01,  1.87000006e-01,  1.77427590e-01,\n         1.06882572e-01, -3.99226040e-01, -1.50639161e-01,\n        -1.27948269e-01,  1.80672511e-01,  5.76634288e-01,\n        -5.58384418e-01,  2.11494833e-01, -3.09537202e-01,\n        -7.94102788e-01,  3.96890789e-02, -1.25942409e-01,\n         3.53242159e-01, -2.66809464e-01,  2.03898683e-01,\n         3.75070393e-01,  6.59319013e-02, -8.80366981e-01,\n         5.60081303e-02, -2.67010331e-01, -1.00177892e-01,\n         8.56251046e-02, -3.48194651e-02, -1.21719539e-01,\n         1.36686653e-01, -8.89018714e-01, -2.71273732e-01,\n         9.41095829e-01,  3.08371246e-01, -2.46233046e-01,\n         5.36555350e-01,  9.04100537e-01,  1.19280107e-02,\n        -7.58319497e-02,  1.68478981e-01,  3.67353320e-01,\n        -7.53687648e-03, -3.97922173e-02, -3.93561542e-01,\n         1.16037987e-02,  9.09914970e-02,  2.97841102e-01,\n        -2.18974769e-01,  1.58638984e-01,  7.61637449e-01,\n         3.44261169e-01, -5.43827951e-01,  8.27776432e-01,\n         3.50882530e-01, -4.21416283e-01,  1.95135355e-01,\n         5.11748195e-01, -8.74435723e-01, -3.06129843e-01,\n        -1.74693167e-01,  3.92624766e-01,  1.63299844e-01,\n         1.46038517e-01, -2.47619092e-01, -2.52310485e-01,\n        -8.06306377e-02,  1.47289038e-01,  4.92957383e-01,\n         7.15941608e-01, -5.77951431e-01, -5.36922887e-02,\n         7.60506034e-01,  4.70994204e-01, -6.82875752e-01,\n        -4.09877539e-01,  1.81261703e-01,  6.91956520e-01,\n         1.98012307e-01,  8.60684216e-01,  5.27474463e-01,\n         4.48377579e-01, -3.50478768e-01,  7.42468894e-01,\n        -3.96050572e-01, -9.46512043e-01, -1.55424386e-01,\n         2.65017878e-02, -1.04338422e-01, -1.54213071e-01,\n        -1.34360999e-01, -6.02925792e-02, -5.00916243e-01,\n         3.59710515e-01, -1.05648920e-01, -5.43551803e-01,\n         1.86915129e-01,  3.04042548e-01,  3.07449847e-01,\n         2.42478341e-01,  9.79310453e-01, -2.59301484e-01,\n        -5.45967698e-01, -4.81588662e-01,  7.59937465e-01,\n        -2.17992187e+00,  1.93750799e-01, -6.92202374e-02,\n        -5.80539629e-02,  3.82068217e-01,  3.03530157e-01,\n        -2.08536014e-01,  7.96037495e-01,  1.44213485e-02,\n        -2.04448048e-02,  1.15865208e-02,  3.21236938e-01,\n         1.00150391e-01,  2.80811787e-01,  1.84053391e-01,\n        -3.42247725e-01, -4.04790789e-01,  7.80040145e-01,\n        -6.03017747e-01,  2.53712028e-01, -2.85416573e-01,\n        -1.11093208e-01,  5.55072546e-01,  6.32149935e-01,\n        -1.18424281e-01, -3.61827537e-02,  2.46493295e-01,\n         2.22678304e-01,  2.17691994e+00, -1.62322119e-01,\n        -5.21773100e-01,  2.41404206e-01,  2.99843639e-01,\n         1.79237604e-01,  2.79523075e-01,  2.16525704e-01,\n         2.35752851e-01, -2.88992375e-02, -1.28362447e-01,\n        -2.50337690e-01, -5.77335417e-01,  7.71387219e-01,\n        -2.23524719e-01, -1.40137643e-01, -7.74832487e-01,\n        -2.07107091e+00, -1.45209700e-01,  1.11701332e-01,\n        -8.78303349e-02, -3.97293046e-02, -3.87312621e-01,\n         3.93610373e-02, -7.45561242e-01, -1.07550219e-01,\n        -4.17522080e-02, -1.01802206e+00,  3.70895922e-01,\n        -8.75935555e-01,  4.73793477e-01, -7.23579168e-01,\n        -2.80746166e-03,  3.75592023e-01, -1.78920627e-01,\n        -5.45527399e-01,  4.26077098e-01, -5.99406719e-01,\n        -1.35499597e-01, -1.46377832e-01, -6.35021925e-01,\n         1.17147155e-01, -6.01645947e-01,  1.41269296e-01,\n         1.28129646e-01, -5.38330078e-01, -8.16933662e-02,\n         1.98015757e-02, -1.45219171e+00, -1.61525711e-01,\n        -7.21467257e-01,  2.57331640e-01,  5.94705105e-01,\n         2.62379825e-01,  4.39873159e-01, -1.51748013e+00,\n         3.62447828e-01,  1.56074122e-01,  8.74011636e-01,\n         3.88701558e-01, -1.84658781e-01,  6.76875040e-02,\n         9.88216046e-03,  1.58232534e+00, -6.56017736e-02,\n        -2.77438760e-01,  3.91076416e-01,  7.51709938e-01,\n        -8.74211341e-02, -8.29604175e-03,  2.64415860e-01,\n         4.94018734e-01, -6.81564286e-02, -2.75227219e-01,\n        -4.82163072e-01, -3.86034071e-01, -1.19277820e-01,\n        -3.57766867e-01,  2.10955635e-01,  4.18056428e-01,\n         1.29322559e-01, -3.08307946e-01,  1.45111114e-01,\n        -9.34398919e-02,  3.78215283e-01, -1.02253366e+00,\n         8.51080865e-02, -3.55230808e-01, -8.00858259e-01,\n        -3.24585974e-01,  1.39626801e+00, -2.90410101e-01,\n         5.38517758e-02,  1.94158554e-01, -1.05355255e-01,\n         2.45212466e-01, -3.55231762e-01, -1.41359672e-01,\n        -1.70282006e-01,  3.10903378e-02,  1.00031398e-01,\n         2.66430378e-01, -1.16066754e-01, -5.85891783e-01,\n        -8.06869417e-02, -5.66609740e-01,  1.60618991e-01,\n        -1.17084765e+00, -5.57788834e-02, -2.68604010e-01,\n         6.68949038e-02, -1.75192654e-01, -4.80566353e-01,\n         6.75415397e-02,  3.82418275e-01, -5.33664703e-01,\n         4.73983645e-01,  1.78156495e-01, -3.52315187e-01,\n        -6.88927770e-01, -2.92324014e-02,  7.38191783e-01,\n        -1.54717684e-01, -2.03011304e-01, -7.10771322e-01,\n         3.65529776e-01,  5.49147427e-01,  3.64255160e-01,\n        -1.50749296e-01, -2.72322088e-01,  1.17936206e+00,\n         5.62639952e-01, -4.39228475e-01,  2.48689055e-01,\n        -3.34329128e-01,  1.16812682e+00, -8.95169854e-01,\n         2.36328512e-01, -8.39320272e-02,  4.84228432e-02,\n        -2.66214341e-01,  4.45342571e-01,  3.01226079e-01,\n         2.76950657e-01, -8.80653560e-02,  1.96803719e-01,\n        -4.68523651e-01, -2.68445164e-01,  1.57110751e-01,\n         4.18580323e-01, -9.98045504e-02,  8.98947418e-02,\n        -2.47603476e-01, -4.57768708e-01,  1.20881602e-01,\n         1.59061193e-01, -9.46967751e-02, -2.66058266e-01,\n        -7.07863152e-01, -3.46234232e-01,  4.90988903e-02,\n         4.27108631e-02, -2.13797078e-01, -2.16978490e-02,\n         3.02777320e-01,  2.49180615e-01,  7.14668989e-01,\n         5.46924055e-01, -1.74997687e-01, -5.33638120e-01,\n        -2.04701930e-01, -5.30350447e-01,  4.55148846e-01,\n         7.65300035e-01, -5.41912243e-02,  1.25864148e-01,\n        -1.69961572e-01,  4.52326924e-01,  3.23951900e-01,\n        -1.49551499e+00,  4.01009172e-01,  1.30639657e-01,\n         1.52500808e-01, -1.60142601e-01,  1.97360560e-01,\n        -2.25259736e-03, -3.98736000e-01, -5.15736461e-01,\n        -1.66302741e-01,  3.53482902e-01, -1.09739077e+00,\n         8.05621520e-02,  1.06688559e-01,  2.11378366e-01,\n        -1.94047347e-01,  5.39112806e-01, -8.85095373e-02,\n         3.25372607e-01, -1.08122015e+00, -1.39681250e-01,\n        -5.35908282e-01, -1.98568121e-01,  1.20834291e-01,\n        -3.13360542e-01,  2.51881212e-01,  1.05706289e-01,\n         3.34290862e-02, -2.99801946e-01, -4.58298206e-01,\n         1.66805446e+00, -3.58683884e-01,  6.62875623e-02,\n         7.49199986e-01, -6.58453852e-02,  2.55714566e-01,\n         5.70453256e-02,  8.19275379e-01,  9.81042027e-01,\n         8.08619484e-02, -7.04527974e-01,  1.51559755e-01,\n        -1.19620718e-01,  6.80505455e-01,  5.25552928e-01,\n         3.68010074e-01,  9.65471208e-01,  1.39987499e-01,\n        -6.93488836e-01,  7.19054639e-02, -2.73527801e-01,\n        -3.85918587e-01,  1.02875330e-01,  2.99414814e-01,\n         5.39037704e-01,  5.99344134e-01,  1.19813599e-01,\n         3.99963260e-01,  2.28384063e-02, -1.03722250e+00,\n        -5.77666223e-01,  5.68650484e-01,  4.85134423e-01,\n         5.95819578e-03, -7.36562312e-01, -7.92311490e-01,\n        -3.06182325e-01, -5.77533126e-01, -2.68044412e-01,\n         2.14768201e-01,  1.66309059e-01, -1.53655529e-01,\n        -6.15000427e-01, -2.17139304e-01, -3.42970192e-02,\n        -2.34937340e-01,  5.71450651e-01, -3.09856027e-01,\n         1.52505994e-01, -1.26403272e-02, -5.27104199e-01,\n        -4.19034272e-01, -2.29210228e-01, -6.52137995e-01,\n        -1.63598073e+00,  2.75931150e-01,  3.25037301e-01,\n         2.74179310e-01,  2.59859145e-01, -4.68724310e-01,\n        -9.90610719e-02,  3.15457284e-02, -1.93704907e-02,\n         3.59142929e-01, -1.78293660e-01, -2.37126857e-01,\n        -2.64783442e-01, -2.41972171e-02,  1.61626741e-01,\n         1.82779729e-02, -9.39925909e-02, -5.92032850e-01,\n         5.17092384e-02,  2.74358690e-01,  3.25071007e-01,\n         1.52489811e-01,  1.96451880e-03, -3.69902998e-01,\n        -1.26375314e-02, -2.61436939e-01,  4.28442955e-01,\n         7.48319328e-02, -3.67506891e-01,  2.35075995e-01,\n         6.74914002e-01, -1.26575932e-01,  6.85960650e-01,\n        -2.27322072e-01,  4.20376509e-01,  3.53540361e-01,\n        -3.35642338e-01,  7.18931556e-01, -5.17799139e-01,\n         7.34502017e-01, -1.53950453e-01,  2.22571597e-01,\n         1.26283765e-01,  4.68970835e-03, -2.35446706e-01,\n         3.64824943e-02, -8.05453807e-02,  2.65444815e-01,\n        -3.77647907e-01,  8.04063305e-02, -6.46169782e-01,\n         2.88563669e-01,  4.89483535e-01,  7.95966625e-01,\n        -3.63395989e-01, -2.32290570e-02,  1.57895952e-01,\n        -5.67564145e-02, -1.49126336e-01,  1.80000588e-01,\n        -6.83725715e-01,  5.16544700e-01,  1.01715341e-01,\n         2.81580567e-01, -9.70489442e-01, -1.10109675e+00,\n         1.50603250e-01,  1.98439866e-01,  1.07275797e-02,\n         5.51607549e-01,  3.51921320e-01, -4.74870145e-01,\n        -2.20645636e-01,  2.31526524e-01, -2.12513044e-01,\n        -8.08937192e-01,  1.35399565e-01, -1.04285049e+00,\n         7.70031065e-02, -7.40196183e-02,  1.06870174e-01,\n         4.10945922e-01,  6.59273714e-02,  2.76734173e-01,\n         5.00406176e-02, -2.15635419e-01, -7.69904315e-01,\n        -8.05577934e-02, -3.64095807e-01,  5.66660643e-01,\n        -1.73553079e-01, -4.84453022e-01,  5.42701304e-01,\n        -9.82382536e-01,  1.51329413e-01, -5.28688030e-03,\n         4.43560839e-01, -1.91310480e-01, -4.55680974e-02,\n        -2.65886605e-01,  3.67179960e-02,  6.14660643e-02,\n         2.79952511e-02, -7.20424533e-01, -4.63774294e-01,\n        -2.54242361e-01,  3.18180919e-02,  7.53512502e-01,\n        -2.81580329e-01,  1.24646711e+00, -1.37928218e-01,\n         3.20327580e-01, -2.21031234e-01,  6.92590326e-02,\n         7.53430128e-01, -9.37000394e-01,  7.95350075e-01,\n         8.29579949e-01, -1.03301001e+00,  4.61388797e-01,\n         7.27596879e-02,  2.11883068e-01,  4.78025138e-01,\n        -5.93761504e-01, -6.10942900e-01,  1.83305120e+00,\n        -5.84538162e-01,  1.15413740e-02, -1.22305870e+00,\n        -4.64831710e-01,  1.52917325e-01,  2.15755120e-01,\n         7.59557039e-02,  7.35588193e-01,  4.08950090e-01,\n        -1.24579653e-01,  7.14068674e-03,  2.13207200e-01,\n        -7.66528130e-01,  4.03783262e-01,  6.80184007e-01,\n         3.74335825e-01, -4.51304615e-02, -2.35138550e-01]], dtype=float32)"
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.encode([\"gym\"]).tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Choosing a model\n",
    "Based on [this](https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search) link we are probably looking at an asymmetric search here so should favour those kinds of models. Fir this, the Bing-derived `msmacro` suite seem to be a good fit ([link](https://www.sbert.net/docs/pretrained-models/msmarco-v3.html)).\n",
    "\n",
    "There is also a need to pick your distance measure, of which there are several but the above links suggest that dot product and cosine are good places to start and cosine is often better for \"short\" descriptions. Do note that Opensearch uses `Approximate Nearest Neighbours` as it's default distance measure which is optimised for fast search, you can override this with cosine but it may slow you down a bit.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [
    "def try_model(mod_name, match_algo, just_nouns=False, psg_in=psg_gym_bench, search_terms=queries):\n",
    "    \"\"\"\n",
    "    This function loads up the descrption passage into a vector and then compares it to the vector of all of the search terms.\n",
    "    Results are presented in descending order.\n",
    "    \"\"\"\n",
    "    if just_nouns:\n",
    "        psg_in = TextBlob(psg_in).noun_phrases\n",
    "        psg_in = \" \".join(psg_in)\n",
    "    model = SentenceTransformer(mod_name)\n",
    "    passage_embedding_1 = model.encode([psg_in])\n",
    "    search_terms = {q: model.encode(q) for q in search_terms}\n",
    "\n",
    "    if match_algo.lower() == \"dot\":\n",
    "        match_func = util.dot_score\n",
    "    elif match_algo.lower() == \"cosine\":\n",
    "        match_func = util.pytorch_cos_sim  # There's also a cos_sim, unsure of diff\n",
    "    else:\n",
    "        raise ValueError(\"Invalid Match Function!\")\n",
    "\n",
    "    return sorted([(float(match_func(v, passage_embedding_1)), k) for k,v in search_terms.items()], reverse=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This one was recommended in the getting started section for semantic search [here](https://www.sbert.net/examples/applications/semantic-search/README.html#) works the same with cosine and dot but it's supposed to be cosine according to the docs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0.6330403089523315, 'weights bench'),\n (0.5748993158340454, 'bench'),\n (0.4849099814891815, 'gym'),\n (0.3846684694290161, 'exercise equipment'),\n (0.3389934301376343, 'weights'),\n (0.19906282424926758, 'exercise'),\n (0.16111910343170166, 'bra'),\n (0.16074158251285553, 'rower'),\n (0.1323268860578537, 'tent'),\n (0.10875529050827026, 'camping'),\n (0.030693121254444122, 'outdoors'),\n (-0.06338250637054443, 'television'),\n (-0.06988093256950378, 'holiday')]"
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_model('multi-qa-MiniLM-L6-cos-v1', \"cosine\") # This was recommeneded in the tutorial"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0.6421118974685669, 'weights bench'),\n (0.6113723516464233, 'bench'),\n (0.43030375242233276, 'gym'),\n (0.3676666021347046, 'weights'),\n (0.2904892861843109, 'exercise equipment'),\n (0.18365596234798431, 'exercise'),\n (0.1546277403831482, 'rower'),\n (0.15009814500808716, 'tent'),\n (0.14600059390068054, 'bra'),\n (0.09983384609222412, 'camping'),\n (0.07700767368078232, 'outdoors'),\n (-0.004586204886436462, 'television'),\n (-0.03594575822353363, 'holiday')]"
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_model('multi-qa-MiniLM-L6-cos-v1', \"cosine\", just_nouns=True) # This was recommeneded in the tutorial"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The documentation for sbert suggests the v3 of this model is good for asynch search (when your queries are short and your documents long). My library couldn't download v3 so I grabbed v2."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0.6697664260864258, 'weights bench'),\n (0.5055959224700928, 'exercise equipment'),\n (0.4895481467247009, 'bench'),\n (0.3891954720020294, 'weights'),\n (0.36898455023765564, 'exercise'),\n (0.21264904737472534, 'bra'),\n (0.16299866139888763, 'camping'),\n (0.09984882175922394, 'rower'),\n (0.06350091099739075, 'gym'),\n (0.009487345814704895, 'outdoors'),\n (-0.0696445032954216, 'holiday'),\n (-0.07525771111249924, 'television'),\n (-0.10196186602115631, 'tent')]"
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_model('msmarco-distilroberta-base-v2', \"cosine\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0.5739995241165161, 'weights bench'),\n (0.4067000150680542, 'weights'),\n (0.38276442885398865, 'exercise equipment'),\n (0.33077865839004517, 'bench'),\n (0.2035306841135025, 'exercise'),\n (0.1936309039592743, 'camping'),\n (0.09358666092157364, 'bra'),\n (0.06547510623931885, 'outdoors'),\n (0.05741633102297783, 'gym'),\n (0.019634557887911797, 'rower'),\n (-0.07226055860519409, 'holiday'),\n (-0.07655307650566101, 'television'),\n (-0.1295216977596283, 'tent')]"
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_model('msmarco-distilroberta-base-v2', \"cosine\", just_nouns=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0.3185180425643921, 'tent'),\n (0.250637948513031, 'outdoors'),\n (0.13116911053657532, 'camping'),\n (0.08360445499420166, 'bench'),\n (0.06664702296257019, 'television'),\n (0.03521207720041275, 'exercise equipment'),\n (0.034268710762262344, 'holiday'),\n (0.0004281101282685995, 'weights bench'),\n (-0.01946995034813881, 'gym'),\n (-0.02163332886993885, 'bra'),\n (-0.02525954321026802, 'rower'),\n (-0.032034821808338165, 'exercise'),\n (-0.05440262332558632, 'weights')]"
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_model('msmarco-distilroberta-base-v2', \"cosine\", psg_in=psg_tent) # trying with tent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a dot optimised model, trying it just to see"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "data": {
      "text/plain": "[(86.560546875, 'weights bench'),\n (64.37287139892578, 'gym'),\n (62.79330825805664, 'bench'),\n (57.10064697265625, 'exercise equipment'),\n (43.40785598754883, 'weights'),\n (41.65951919555664, 'exercise'),\n (29.711671829223633, 'rower'),\n (29.216854095458984, 'bra'),\n (14.020475387573242, 'tent'),\n (13.067646026611328, 'outdoors'),\n (9.146438598632812, 'camping'),\n (3.8990843296051025, 'television'),\n (-23.02558135986328, 'holiday')]"
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot optimised\n",
    "try_model('msmarco-distilbert-base-v4', \"dot\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [
    {
     "data": {
      "text/plain": "[(60.13253402709961, 'weights bench'),\n (41.857872009277344, 'bench'),\n (37.81570816040039, 'weights'),\n (36.69390869140625, 'exercise equipment'),\n (36.47909927368164, 'gym'),\n (16.717876434326172, 'rower'),\n (10.237001419067383, 'exercise'),\n (5.496613502502441, 'bra'),\n (3.109081506729126, 'tent'),\n (-1.6696975231170654, 'outdoors'),\n (-3.579359531402588, 'camping'),\n (-9.178805351257324, 'television'),\n (-26.839519500732422, 'holiday')]"
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_model('msmarco-distilbert-base-v4', \"dot\", just_nouns=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reranking\n",
    "This may be somethign worth looking into, I've not at this point. [Link](https://www.sbert.net/examples/applications/retrieve_rerank/README.html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Opensearch\n",
    "To see how this works IRL I've plumbed this into an opensearch instance to see if it works. This is an older version that I had lying around, I'd def suggest you use the latest as this functionality is quite new. Documentation exists [here](https://opensearch.org/), and the K-nearest Neighbours (KNN) implementation that will find you the closest match is documented [here](https://opensearch.org/docs/latest/search-plugins/knn/index/).\n",
    "\n",
    "The different ways it searches are documented [here](https://opensearch.org/docs/latest/search-plugins/knn/approximate-knn/#spaces), the default (I think) is `L1` so to get optimum performance on the above models you probably want to flip it to `cosinesimil`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [],
   "source": [
    "# Sorry, not sharing my login here!\n",
    "with open(r\"C:\\Users\\robert.mansfield\\.passes\\test_es.json\", \"r\") as f:\n",
    "    creds = json.loads(f.read())\n",
    "\n",
    "es = OpenSearch(hosts=creds[\"host\"], http_auth=(creds['user'], creds['pass']))\n",
    "\n",
    "my_idx = 'rob_semantic_test'\n",
    "# es.cat.indices()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [
    {
     "data": {
      "text/plain": "768"
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This model did pretty well above but it does give a pretty big vector to search!\n",
    "m = SentenceTransformer('msmarco-distilbert-base-v4')\n",
    "len(m.encode(psg_gym_bench).tolist())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [
    {
     "data": {
      "text/plain": "{'acknowledged': True,\n 'shards_acknowledged': True,\n 'index': 'rob_semantic_test'}"
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create index. Should probably try to activate the cosine similarity at some point. Think my test ES is too old\n",
    "try:\n",
    "    es.indices.delete(index=my_idx)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "idx = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": len(m.encode(psg_gym_bench).tolist()),  # This is the length of your model's output vector.\n",
    "                # \"method\":{\"space_type\": \"cosinesimil\"}  # I think this is how you set this but you need a newer Opensearch than I have to hand.\n",
    "            },\n",
    "            \"description\": {\n",
    "                \"type\": \"text\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "es.indices.create(index=my_idx, body=idx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [
    {
     "data": {
      "text/plain": "{'_index': 'rob_semantic_test',\n '_type': '_doc',\n '_id': '2',\n '_version': 1,\n 'result': 'created',\n '_shards': {'total': 2, 'successful': 2, 'failed': 0},\n '_seq_no': 0,\n '_primary_term': 1}"
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add some documents\n",
    "es.create(index=my_idx,\n",
    "          body={\"vector\": m.encode(psg_gym_bench).tolist(), \"description\": psg_gym_bench},\n",
    "          id=1\n",
    "          )\n",
    "\n",
    "es.create(index=my_idx,\n",
    "          body={\"vector\": m.encode(psg_tent).tolist(), \"description\": psg_tent},\n",
    "          id=2\n",
    "          )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [],
   "source": [
    "def do_query(term):\n",
    "    \"\"\"\n",
    "    Do a search based on distance to the given vector. Return just the score and the description\n",
    "    \"\"\"\n",
    "    qry = {\n",
    "        \"size\": 2,  # Max results\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                \"vector\": {  # This is the one with your column name, I stupidly called my column vector!\n",
    "                    \"vector\": m.encode(term).tolist(),\n",
    "                    \"k\": 2 # Max results per shard\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    res = es.search(qry, index=my_idx)\n",
    "    return [(h['_source'][\"description\"], h[\"_score\"]) for h in res[\"hits\"][\"hits\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [
    {
     "data": {
      "text/plain": "[('The BodyMax CF302 Flat Bench with Dumbbell Rack allows you to create an exciting and varied workout to help strengthen, tone and promote weight loss, whilst also keeping your gym floor clutter free! Stylish and classic, this flat bench has been thoughtfully constructed with durable upholstery and high-density padding to ensure maximum comfort while you train.',\n  0.004655081),\n ('The Theta 4 Tent is a four-person tunnel tent, uniquely featuring two sleeping cabins and a moveable front wall, allowing you to choose whether you have a covered porch or a larger living area inside your tent. With such an incredible size of 340(W) x 480(D) x 190cm(H), the Theta 4 has more than plenty of space inside – perfect for a family getaway, a couples’ retreat, or a trip shared among friends.',\n  0.002962307)]"
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_query(\"gym\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "data": {
      "text/plain": "[('The Theta 4 Tent is a four-person tunnel tent, uniquely featuring two sleeping cabins and a moveable front wall, allowing you to choose whether you have a covered porch or a larger living area inside your tent. With such an incredible size of 340(W) x 480(D) x 190cm(H), the Theta 4 has more than plenty of space inside – perfect for a family getaway, a couples’ retreat, or a trip shared among friends.',\n  0.005420531),\n ('The BodyMax CF302 Flat Bench with Dumbbell Rack allows you to create an exciting and varied workout to help strengthen, tone and promote weight loss, whilst also keeping your gym floor clutter free! Stylish and classic, this flat bench has been thoughtfully constructed with durable upholstery and high-density padding to ensure maximum comfort while you train.',\n  0.0031871875)]"
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_query(\"tent\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [
    {
     "data": {
      "text/plain": "[('The BodyMax CF302 Flat Bench with Dumbbell Rack allows you to create an exciting and varied workout to help strengthen, tone and promote weight loss, whilst also keeping your gym floor clutter free! Stylish and classic, this flat bench has been thoughtfully constructed with durable upholstery and high-density padding to ensure maximum comfort while you train.',\n  0.0044708215),\n ('The Theta 4 Tent is a four-person tunnel tent, uniquely featuring two sleeping cabins and a moveable front wall, allowing you to choose whether you have a covered porch or a larger living area inside your tent. With such an incredible size of 340(W) x 480(D) x 190cm(H), the Theta 4 has more than plenty of space inside – perfect for a family getaway, a couples’ retreat, or a trip shared among friends.',\n  0.0029711577)]"
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the description doesn't actually include the word exercise anywhere!\n",
    "do_query(\"exercise equipment\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [
    {
     "data": {
      "text/plain": "[('The Theta 4 Tent is a four-person tunnel tent, uniquely featuring two sleeping cabins and a moveable front wall, allowing you to choose whether you have a covered porch or a larger living area inside your tent. With such an incredible size of 340(W) x 480(D) x 190cm(H), the Theta 4 has more than plenty of space inside – perfect for a family getaway, a couples’ retreat, or a trip shared among friends.',\n  0.005420531),\n ('The BodyMax CF302 Flat Bench with Dumbbell Rack allows you to create an exciting and varied workout to help strengthen, tone and promote weight loss, whilst also keeping your gym floor clutter free! Stylish and classic, this flat bench has been thoughtfully constructed with durable upholstery and high-density padding to ensure maximum comfort while you train.',\n  0.0031871875)]"
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_query(\"tent\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [
    {
     "data": {
      "text/plain": "[('The BodyMax CF302 Flat Bench with Dumbbell Rack allows you to create an exciting and varied workout to help strengthen, tone and promote weight loss, whilst also keeping your gym floor clutter free! Stylish and classic, this flat bench has been thoughtfully constructed with durable upholstery and high-density padding to ensure maximum comfort while you train.',\n  0.0045321896),\n ('The Theta 4 Tent is a four-person tunnel tent, uniquely featuring two sleeping cabins and a moveable front wall, allowing you to choose whether you have a covered porch or a larger living area inside your tent. With such an incredible size of 340(W) x 480(D) x 190cm(H), the Theta 4 has more than plenty of space inside – perfect for a family getaway, a couples’ retreat, or a trip shared among friends.',\n  0.0027877921)]"
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_query(\"bench\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [
    {
     "data": {
      "text/plain": "[('The Theta 4 Tent is a four-person tunnel tent, uniquely featuring two sleeping cabins and a moveable front wall, allowing you to choose whether you have a covered porch or a larger living area inside your tent. With such an incredible size of 340(W) x 480(D) x 190cm(H), the Theta 4 has more than plenty of space inside – perfect for a family getaway, a couples’ retreat, or a trip shared among friends.',\n  0.0027689987),\n ('The BodyMax CF302 Flat Bench with Dumbbell Rack allows you to create an exciting and varied workout to help strengthen, tone and promote weight loss, whilst also keeping your gym floor clutter free! Stylish and classic, this flat bench has been thoughtfully constructed with durable upholstery and high-density padding to ensure maximum comfort while you train.',\n  0.0027312732)]"
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_query(\"cat\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [
    {
     "data": {
      "text/plain": "[('The Theta 4 Tent is a four-person tunnel tent, uniquely featuring two sleeping cabins and a moveable front wall, allowing you to choose whether you have a covered porch or a larger living area inside your tent. With such an incredible size of 340(W) x 480(D) x 190cm(H), the Theta 4 has more than plenty of space inside – perfect for a family getaway, a couples’ retreat, or a trip shared among friends.',\n  0.002747939),\n ('The BodyMax CF302 Flat Bench with Dumbbell Rack allows you to create an exciting and varied workout to help strengthen, tone and promote weight loss, whilst also keeping your gym floor clutter free! Stylish and classic, this flat bench has been thoughtfully constructed with durable upholstery and high-density padding to ensure maximum comfort while you train.',\n  0.0025693618)]"
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_query(\"holiday\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}